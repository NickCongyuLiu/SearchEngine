COMPETING WITH GOOGLE



First, let's consider a simple description of the internet.  The

internet consists of *web pages* which are actually just files.  There

are many types of files, but the ones which interest us are written in

HTML (HyperText Markup Language).  Each web page is accessed by its

URL (Uniform (or universal) Resource Locator), which looks like this:



http://www.cs.miami.edu/~vjm/csc220/index.html



An HTML file has words (text) on it and URLs (links), which are

written in a special way:



Here is some 

<A HREF=http://download.oracle.com/javase/6/docs/api/>

Java Documentation</A>



This makes the words Java Documentation appear, but when you click on

it, it goes to the web page with URL



http://download.oracle.com/javase/6/docs/api/



By the way, if no file is mentioned, the default is index.html.  If

there is no index.html, it just lists the directory, like every time

you go to one of my prog directories.



When we give some search words to Google, such as



Victor Milenkovic Java



Google finds all web pages which contain those words and ranks them by

significance.  It's a lot more complicated now, but originally the

significance was determined by the number of references (links) from

other web pages on the internet.



The idea is that if a web page is "good", then people will "vote" for

it by putting links to it in their web pages.  If this were done

honestly, then the original idea would work fine.  Of course, people

try to subvert this idea for fun or profit by creating lots of "dummy"

web pages that link to a page they want people to go to.  It's like

stuffing the ballot box in the old days of voting.





INDEXING WEB PAGES



Obviously, Google doesn't do the search by going out on the internet

when you make a search request.  Instead, they gather up information

on web pages ahead of time and store this information on their own

servers.  They have to store it in a way that facilitates rapid

search.  This is often called *indexing*.  We now know enough

techniques to have a shot at explaing how Google manages to organize

their information in a way that allows searches in a fraction of a

second.



To start with, *indexing* is not mysterious at all.  Every time we see

a new web page, we will add it to our master list of all the web pages

we have seen so far



List<String> pagesSeen;



If we add "http://www.cs.miami.edu/~vjm/index.html" to this list and

if there were 1000 URLs in the list already, then

"http://www.cs.miami.edu/~vjm/index.html" would be at index 1000

because the previous ones have indices 0 to 999.



**How do we add a new item to the end a List in Java?

  (** means you need to know this for lab or the quiz.)



**How can we tell what index it just got?



Right away, there is a problem.  The list pagesSeen should not have

duplicates.  Since we don't want to change the index of a page, we

can't even sort it.  So it takes O(n) to determine if a page is already

in the list.  Potentially, n is REALLY BIG like a trillion.



So we need another data structure that can tell us the index of a page

(if it has one) and do it quickly.  It should take a URL and give us

an index.  What interface are we talking about?



We have been converting strings to numbers since prog02.  We know that

the formal name for this interface is Map.  We need another data

structure:



Map<String, Integer> pageToIndex;



**Right after we add a new URL to pagesSeen, how do we put the new URL

  and index into pageToIndex?



We also need to keep track of the significance of each web page, which

for simplicity is the number of references to it from other web pages.

We may not have seen these yet, so initially the reference count is

zero.  We store the reference counts in a List parallel to pagesSeen.



List<Integer> refCounts;



**Right after add a new URL to pagesSeen, how do we add its initial

  reference count to refCounts?





INDEXING WORDS



We need to keep track of all the words we see and index them too.  So

we have another list:



List<String> wordsSeen;



We also need to look up the index of a word quickly:



Map<String, Integer> wordToIndex;



What else do we need to store for each word?  In order to answer a

query like "Victor Milenkovic Java", we need to know the web pages

which contain Victor, those which contain Milenkovic, and those which

contain Java.  (Then we will take the intersection of these three

lists.)  So for each word, we need to know the list of web pages which

have that word.  For instance, we need to know the list of web pages

which have Milenkovic on them.



What do we mean by a list of web pages?  It could be a list of URLs or

a list of web page (url) indices.  The latter is much more compact.

So for each word, we will have a list of the indices of web pages

which contain it.  Each word has a List<Integer>.  Where can we store

all these lists?  In a List!!  Initially, the list for a word is

empty.  We will use ArrayList as the implementation for all our lists.



List< List<Integer> > pageIndexLists;



**Right after you add a new word to wordsSeen, how do you add its

  initial list to pageIndexLists?



**If you discover that the word with index wordIndex is on the web

  page with index urlIndex, how do you add urlIndex to the list of

  indices belonging to that word?



GATHERING ALL WEB PAGES



Every year GoDaddy has a superbowl ad.  For years, the commercials had

nothing to do with the product, culminating in Bar Refaeli kissing

Jesse Heiman in 2013.  This year, the ads finally gave some hint as to

what GoDaddy sells.



GoDaddy is a domain name registrar.  So when my wife wanted to create

sleuthacademy.org, she paid GoDaddy some money and there is was.  (She

also has to host it somewhere.)



People register about one new domain PER SECOND, most with GoDaddy.

So you can understand how they could afford to pay Jesse.



Each domain has multiple web pages, and Google wants to index them

all.  So every day, GoDaddy tells Google about the new domains and

Google indexes all their web pages and everything they link to.



How can we do the same thing?  We get a bunch of URLs.  For each one,

check to make sure we haven't seen it before (**how?).  If not, index

it and put it in a Queue.



While the Queue is not empty, take out a URL.  Using the Browser I

provide you, get the List of words and URLs on that page.  For each

new URL, index it and put it in the queue.  Index each new word.  Add

the index of the current page (the one we just dequeued) to the list

for each word.  Increment the reference count for every page that

referenced.



IMPLEMENTATIONS



We are not actually going to gather a trillion web pages, but the

point is that there are a lot of them out there.  If we did, we would

need to store them on hard disks.  They would not fit into memory.

What implementations should we use for wordsSeen, pageToIndex, and

refCounts?



refCounts is the simplest.  Each entry is an integer, which is four

bytes.  For a trillion web pages, that is 4TB.  Lots of disks are that

big.  The integers are in one enormous file.  When we index a new

page, we add its (zero) reference count to the end of the file using

one disk seek.  To look up the reference count for the page with index

i, read in disk page number i / 256 as an array of integers and go to

index i % 4 in that array.  Why 256?  Because a disk page is 1024 = 256*4

bytes.  That takes one disk seek.  Incrementing is similar.



pagesSeen is a little more complicated since URLs are different lengths

in bytes.  Also, we would probably want to compress them.  Are we

going to waste four bytes to store ".com" each time?  In any case, we

would write them to an enormous file.  We might need more than one

disk.  We could start each disk block with the starting index as the

first eight bytes. (Why eight?  Four bytes is not enough to store a

trillion.)  We would use a form of interpolation search to find the

right disk block.



pageToIndex is the most complicated still.  We would use a 32-63 b-tree.

Since 32 = 2^5, 32^2 is about 1000.  8 levels in the tree will

allow you to have 32^8 = 1024^4 or about a trillion leaves.  So you

can look up any name using 8 disk seeks.  Or less if you read in

bunches of pages.



To really make it efficient, you should alphabetize by backward domain name:



http://www.cs.miami.edu/~vjm/csc220/prog06/lab.txt



should be something like



edu.miami.cs.www/~vjm/csc220/prog06/lab.txt



Can you see why?



You might notice that this is a lot longer than 8 bytes, which is the

space we have alloted for each key in the entry.  That's o.k. because

we can leave off a common prefix and just store the parts that are

different.



A B-Tree is also good because it is sorted.  Sorted is good because

web pages tend to point to other web pages which are nearby

alphabetically (right?).  So you are likely to be be nearby in the

B-Tree.  The idea is to save the path to the previous page to help when

you look up the next one.  So even though lookup is O(log n), it is

really O(1) if you use this trick.



Since it might take eight disk seeks to look up a page in a B-Tree, you

might always want to have a hash table (in memory) containing all the

very common pages.  When you see a page, look it up simultaneously in

the B-Tree and the hash table.  If it's in the hash table, stop the

B-Tree search.



wordsSeen and wordToIndex are much smaller because there are only

millions of words out there.  So we can store them in memory.  Since

we do not encounter words in alphabetical order (usually), a hash

table makes the most sense for wordToIndex.



Each list in pageIndexLists would really be a file on a hard disk.  So

what we would store in memory is a list of pointers to these file.



LAB IMPLEMENTATION



Use TreeMap for pageToIndex, HashMap for wordToIndex, and ArrayList for

all the lists.





Note on 12.03



Iterators

The search method is given a list of key words:  "Victor", "Milenkovic",  "Java".  Associated with each key word is the list of page indices of pages which contain that word.  In the real world, these lists would be very large and have to be stored on disk.  So when you are reading them in, you would start at the beginning and read to the end as the disk spins.  You would not jump around.  So no using get(index) for these lists!

 

An appropriate model for reading a file is an Iterator.  The hasNext()  method tells you if you are at the end of the file.  The next() method  reads in the next element.  There is no going back.  What is happening  behind the scenes is that the files are being loaded in kilobyte blocks  and the CPU is so fast that it is processing the input as fast as it can get it.  In fact, the CPU is doing other things while the next() is  being executed.  Assuming that the three lists are on different disks,  the processing is happening as fast as the disks can be read.  This is  very fast once the read head of the disk is at the correct position:   the disk spins and the data streams into the CPU.

 

So you must use an Iterator for each list.  Since there are multiple keywords, you need an array Iterator<Integer>[].  (Actually, it would be Long in real life.)

 

I have given you the incantation to create this  array, but its entries are all null.  You need to initialize each entry  to the appropriate iterator.  Where do you get this iterator?

 

 

Looking for Matches

 

If you call next() on an Iterator, you better store the value somewhere  because you can never go back again.  So you need an array  currentPageIndices to hold the current page indices:  the ones you most recently read from each Iterator.  How do you update this array?

Suppose the current page ids for "Victor", "Milenkovic", and "Java" are 111, 123, and 11.  Because the "Milenkovic" list is in increasing order and the current entry is 123, the ids 111 and 11 cannot appear in that list.  So we can safely do a next() on the "Victor" list and the "Java" list without fear of skipping past matching ids.  The rule is that we can advance all but the largest entry (or entries, it can appear more than once).

However if all entries are the same, then that's a match, which we presumably detected, so we should move forward on all the lists.

moveForward implements this strategy:  if all are equal, move all forward one; if not, move all non-largest entries forward one.

 

In what follows, the first column represents the contents of an array containing the page indices you most recently read from each ``file''.  The rest of each row represents the page indices remaining in the ``file''.  Of course, the ``file'' is just an ArrayList for us, but in real life it would be a file on disk.

 

0   111 222 333 444 555 666 777

0   123 444 666 1234

0   11 22 33 44 444 555 600 666 678 880 888

 

111   222 333 444 555 666 777

123   444 666 1234

11     22 33 44 444 555 600 666 678 880 888

 

222   333 444 555 666 777

123   444 666 1234

22     33 44 444 555 600 666 678 880 888

 

222   333 444 555 666 777

444   666 1234

44     444 555 600 666 678 880 888

 

333   444 555 666 777

444   666 1234

444   555 600 666 678 880 888

 

444   555 666 777

444   666 1234

444   555 600 666 678 880 888

And we have found a match!

 

555   666 777

666   1234

555   600 666 678 880 888

 

666   777

666   1234

600   666 678 880 888

 

666   777

666   1234

666   678 880 888

Found a match!

 

777

1234

678    880 888

 

We want to advance the first list, but it has no more elements, so there can be no more matches.

 

PriorityQueue

 

As you know, Java provides an implementation of the heap data structure called  PriorityQueue.  We need to put URL indices into a PriorityQueue.  Why?  Because we need to keep track of the best 10 matching pages.

 

If we already have 10 matching pages, we should compare the current matching page to the ``top'' of the heap.  If it is better, take out the top of the heap and put this match in.  If we don't yet have 10 matching pages, just put this one in.

 

HOWEVER, we do not want to compare pages based on their indices!   Which is better?

cs.miami.edu with index 1111 and reference count 9999

sleuthacademy.org with index 7777 and reference count 5555

So you need to create a Comparator class for the PriorityQueue to use.  You know how to do that!  Is compare(1111, 7777) positive or negative?  Why?  Answer:  it is positive because 9999 is larger than 5555.

 

The PriorityQueue has the LEAST significant page at the ``top''.  Why?  If I want to get into the top 10, do I need to beat the best or the 10th best?

Steve and Mark are camping when a bear suddenly comes out and growls.  Steve starts putting on his tennis shoes.

Mark says, ¡°What are you doing? You can¡¯t outrun a bear!¡±

Steve says, ¡°I don¡¯t have to outrun the bear¡ªI just have to outrun you!¡±

 

That means that when we are done and dequeue the indices from the PriorityQueue, we will get the top 10 in order from WORST to BEST.  But we want to put them into the output array from BEST to WORST.  What to do?

EXAMPLE:

Here are matching page indices with refcounts in parentheses.

444(345), 666(222), 789(223), 987(321), 1001(200), 1234(300), 2001(400)

Let's do the example with 3 outputs instead of 10.



Priority Queue

EMPTY

Add 444?  size < 3, so yes.

444

Add 666?  size < 3, so yes.  Notice 666 comes before 444.  Why?

666 444

Add 789?  size < 3, so yes.  Why is 666 still on "top"?

666 789 444

Add 987?  size == 3.  peek() returns 666.  987 has higher priority (321) than 666 (222), so poll() out 666 and offer(987).  Why is 987 in the middle?

789 987 444

Add 1001?  size == 3.  peek() returns 789.  1001 has lower priority (200) than 789 (223), so no.

789 987 444

Add 1234?  size == 3.  peek() returns 790.  1234 has higher priority (300) than 789 (223), so poll() and then offer(1234).  Why is 1234 on top?

1234 987 444

Add 2001?  size == 3.  peek() returns 1234.  2001 has higher priority (400) than 1234 (300), so poll() tand then offer(2001).

987 444 2001

Now repeatedly poll and fill the output array in reverse order:

Priority Queue: 987 444 2001

Output Array: null null null

poll():

Priority Queue: 444 2001

Output Array: null null 987

poll():

Priority Queue: 2001

Output Array: null 444 987

poll():

Priority Queue: 

Output Array: 2001 444 987

So the output is the top three matching pages in order of decreasing number of references.


Of course, you should put their URLs in the output array, not their indices.